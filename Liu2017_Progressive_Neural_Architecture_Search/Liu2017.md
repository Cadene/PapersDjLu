## Cell Topologies (3.1)

[See Figure-1 left]

A **cell** is a Fully Convnet represented by a Directed Acyclic Graph (DAG) of B blocks. The output of a cell is the concatenation of all the outputs of its ending blocks (those ending blocks do not give their outputs to other blocks in the same cell).

A **block** is a tuple $(I_1, I_2, O_1, O_2, C)$ with I the inputs, O the operations on inputs, C the operations to combine the resulting outputs.

The set of possible inputs I is the outputs set of:
- all previous blocks
- previous cell
- previous previous cell

The set of possible operations O is:
- identity
- 3x3 avg pool
- 3x3 max pool
- 3x3 dilated conv
- 1x7 followed by 7x1 conv
- 3x3 depthwise-separable conv
- 5x5 depthwise-separable conv
- 7x7 depthwise-separable conv
- (they removed the ones that NASNet did not find useful)

The set of possible operation C is:
- element wise addition
- (they removed the concatenation because NASNet did not find it useful)

The number of possible architectures for the b'th block:
$$ |B_b| = |I|^2 \times |O|^2 \times |C| $$
$$ (2+b-1)^2 * 8^2 * 1 $$

- for one block: $ |B_1| = 256 $
- for 2 block: $ |B_{1:2}| = 256 * 576 = 147456$
- for 5 blocks: $ |B_{1:5}| = 5.6 * 10^{14} $
- for 5 blocks, taking the symmetry into account: $ |B^s_{1:5}| \sim 10^{12} $
- NASNet has $10^{28}$ possible structures

## From Cell to Convnet (3.2)

[See Figure-1 right]

A basic cell (same architecture) is stacked N times using either stride 1 or 2 (in a cyclic manner, 1-2-1-2-1 for CIFAR-10 or 2-1-2-1-2-1 for ImageNet). Avg pooling at the top before last classification layer.

*Note: They employ a heuristic to double the number of filters (feature maps) whenever the spatial activation is halved.* $|F_{out}| = s * |F_{in}|$ if $s$ stride.

Input size is decided before hand between 32x32 for CIFAR-10, 224x224 or 331x331 for ImageNet.

A initial 3x3 conv with stride 2 is added at the start of ImageNet network to reduce the input size, thus the overall cost.

*Note: In NASNet they have Normal and Reduction cells. Here it is emulated by the stride (1 for Normal, 2 for Reduction). Thus, the number of possible architectures is reduced.*

## Progressive Neural Architecture Search (4.1)

[See Algorithm-1]

They start by constructing all possible $B_1$ architectures (1 block) and then expand each one by adding all the possible $B_{1:2}$ architectures.

They use the following hyperparameters:
- max number of blocks (b)
- max number of epochs
- number of filters in the first layer (F)
- number of possible jobs (K)
- number of stacked cells (N)

Trained by REINFORCE.

## Performance Prediction with Surrogate Model (4.2)

[See Figure-2]

Since $|B_{1:2}| = 147456$ is too big, they learn an arch predictor (based on an RNN) to predict the scores of the arch from the string definition.  They train it on the $B_1$ architectures results. They apply it on the $B_{1:2}$ and take the K most promising ones. They repeat the process, until they find cells with a sufficient number of blocks.


## Discussion and Future Work (6)

May be interesting.
