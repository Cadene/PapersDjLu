@article { Shih2015,
	abstract = {We present a method that learns to answer visual questions by selecting image
regions relevant to the text-based query. Our method exhibits significant
improvements in answering questions such as "what color," where it is necessary
to evaluate a specific location, and "what room," where it selectively
identifies informative image regions. Our model is tested on the VQA dataset
which is the largest human-annotated visual question answering dataset to our
knowledge.},
	url = {http://arxiv.org/pdf/1511.07394v2},
	eprint = {1511.07394},
	arxivid = {1511.07394},
	archiveprefix = {arXiv},
	month = {Nov},
	year = {2015},
	booktitle = {arXiv},
	title = {Where To Look: Focus Regions for Visual Question Answering},
	author = {Kevin J. Shih and Saurabh Singh and Derek Hoiem}
}

