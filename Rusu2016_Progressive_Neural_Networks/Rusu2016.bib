@article { Rusu2016,
	abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer
and avoiding catastrophic forgetting--remains a key obstacle to achieving
human-level intelligence. The progressive networks approach represents a step
forward in this direction: they are immune to forgetting and can leverage prior
knowledge via lateral connections to previously learned features. We evaluate
this architecture extensively on a wide variety of reinforcement learning tasks
(Atari and 3D maze games), and show that it outperforms common baselines based
on pretraining and finetuning. Using a novel sensitivity measure, we
demonstrate that transfer occurs at both low-level sensory and high-level
control layers of the learned policy.},
	url = {http://arxiv.org/pdf/1606.04671v3},
	eprint = {1606.04671v3},
	arxivid = {1606.04671v3},
	archiveprefix = {arXiv},
	month = {Jun},
	year = {2016},
	booktitle = {arXiv},
	title = {Progressive Neural Networks},
	author = {Andrei A. Rusu and Neil C. Rabinowitz and Guillaume Desjardins and Hubert Soyer and James Kirkpatrick and Koray Kavukcuoglu and Razvan Pascanu and Raia Hadsell}
}

