@article { Neyshabur2018,
	abstract = {Despite existing work on ensuring generalization of neural networks in terms
of scale sensitive complexity measures, such as norms, margin and sharpness,
these complexity measures do not offer an explanation of why neural networks
generalize better with over-parametrization. In this work we suggest a novel
complexity measure based on unit-wise capacities resulting in a tighter
generalization bound for two layer ReLU networks. Our capacity bound correlates
with the behavior of test error with increasing network sizes, and could
potentially explain the improvement in generalization with
over-parametrization. We further present a matching lower bound for the
Rademacher complexity that improves over previous capacity lower bounds for
neural networks.},
	url = {http://arxiv.org/pdf/1805.12076v1},
	eprint = {1805.12076},
	arxivid = {1805.12076},
	archiveprefix = {arXiv},
	month = {May},
	year = {2018},
	booktitle = {arXiv},
	title = {{Towards Understanding the Role of Over-Parametrization in Generalization
  of Neural Networks}},
	author = {Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro}
}

