@article { Henriques2016,
	abstract = {Convolutional Neural Networks (CNNs) are extremely efficient, since they
exploit the inherent translation-invariance of natural images. However,
translation is just one of a myriad of useful spatial transformations. Can the
same efficiency be attained when considering other spatial invariances? Such
generalized convolutions have been considered in the past, but at a high
computational cost. We present a construction that is simple and exact, yet has
the same computational complexity that standard convolutions enjoy. It consists
of a constant image warp followed by a simple convolution, which are standard
blocks in deep learning toolboxes. With a carefully crafted warp, the resulting
architecture can be made invariant to one of a wide range of spatial
transformations. We show encouraging results in realistic scenarios, including
the estimation of vehicle poses in the Google Earth dataset (rotation and
scale), and face poses in Annotated Facial Landmarks in the Wild (3D rotations
under perspective).},
	url = {http://arxiv.org/pdf/1609.04382v1},
	eprint = {1609.04382v1},
	arxivid = {1609.04382v1},
	archiveprefix = {arXiv},
	month = {Sep},
	year = {2016},
	booktitle = {arXiv},
	title = {Warped Convolutions: Efficient Invariance to Spatial Transformations},
	author = {Jo√£o F. Henriques and Andrea Vedaldi}
}

