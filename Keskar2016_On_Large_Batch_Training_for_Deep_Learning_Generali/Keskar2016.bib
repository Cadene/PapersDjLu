@article { Keskar2016,
	abstract = {The stochastic gradient descent method and its variants are algorithms of
choice for many Deep Learning tasks. These methods operate in a small-batch
regime wherein a fraction of the training data, usually $32$--$512$ data
points, is sampled to compute an approximation to the gradient. It has been
observed in practice that when using a larger batch there is a significant
degradation in the quality of the model, as measured by its ability to
generalize. There have been some attempts to investigate the cause for this
generalization drop in the large-batch regime, however the precise answer for
this phenomenon is, hitherto unknown. In this paper, we present ample numerical
evidence that supports the view that large-batch methods tend to converge to
sharp minimizers of the training and testing functions -- and that sharp minima
lead to poorer generalization. In contrast, small-batch methods consistently
converge to flat minimizers, and our experiments support a commonly held view
that this is due to the inherent noise in the gradient estimation. We also
discuss several empirical strategies that help large-batch methods eliminate
the generalization gap and conclude with a set of future research ideas and
open questions.},
	url = {http://arxiv.org/pdf/1609.04836v1},
	eprint = {1609.04836v1},
	arxivid = {1609.04836v1},
	archiveprefix = {arXiv},
	month = {Sep},
	year = {2016},
	booktitle = {arXiv},
	title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp
  Minima},
	author = {Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang}
}

