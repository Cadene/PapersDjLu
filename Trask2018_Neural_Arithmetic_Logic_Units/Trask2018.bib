@article { Trask2018,
	abstract = {Neural networks can learn to represent and manipulate numerical information,
but they seldom generalize well outside of the range of numerical values
encountered during training. To encourage more systematic numerical
extrapolation, we propose an architecture that represents numerical quantities
as linear activations which are manipulated using primitive arithmetic
operators, controlled by learned gates. We call this module a neural arithmetic
logic unit (NALU), by analogy to the arithmetic logic unit in traditional
processors. Experiments show that NALU-enhanced neural networks can learn to
track time, perform arithmetic over images of numbers, translate numerical
language into real-valued scalars, execute computer code, and count objects in
images. In contrast to conventional architectures, we obtain substantially
better generalization both inside and outside of the range of numerical values
encountered during training, often extrapolating orders of magnitude beyond
trained numerical ranges.},
	url = {http://arxiv.org/pdf/1808.00508v1},
	eprint = {1808.00508},
	arxivid = {1808.00508},
	archiveprefix = {arXiv},
	month = {Aug},
	year = {2018},
	booktitle = {arXiv},
	title = {{Neural Arithmetic Logic Units}},
	author = {Andrew Trask and Felix Hill and Scott Reed and Jack Rae and Chris Dyer and Phil Blunsom}
}

