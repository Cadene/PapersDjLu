It aligns words to imagepatches in the first hop, and then refers to the entire question for obtaining image attention maps inthe second hop.



## Tache


VQA (classification parmi K classes)

Comparaison avec Show Attend and Tell

Demo de iBOWIMG




## Contributions 

A major drawback of existing models is that they do not have any explicit notion of object position and do not support the computation of intermediate results based on spatial attention
-> Spatial Memory Network VQA (SMem-VQA)
Image figure 1

ils ont utilisé Caffe

Inspiré de Memory Network qui a récemment été proposé pour QA.
-> knowledge forme texte
-> VQA knowledge forme image

design an attention architecture in the first hop which uses each word embedding to capture fine-grained alignment between the image and question

- design synthetic questions that specifically require spatial inference and visualize the attention weights
- evaluation on DAQUAR and VQA, improve results compared to iBOWIMG which concatenates image and question features

## Related work

Explique QA et iBOWIMG

## Models


##  Experiments

### Exploring Attention on Synthetic Data

Absolute Position Recognition

Relative Position Recognition

### Experiments o,n Standard Datasets

DAQUAR

VQA

## Conclusion

Model improves results over previously published models
Modle can be used to visualize the inference steps learned by the deep network, giving some insight into its processing

future work may include further exploring the inference ability and exploring other VQA attention models
