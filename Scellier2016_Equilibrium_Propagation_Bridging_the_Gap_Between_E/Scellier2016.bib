@article { Scellier2016,
	abstract = {We introduce Equilibrium Propagation (e-prop), a learning algorithm for
energy-based models. This algorithm involves only one kind of neural
computation both for the first phase (when the prediction is made) and the
second phase (after the target is revealed) of training. Contrary to
backpropagation in feedforward networks, there is no need for special
computation in the second phase of our learning algorithm. Equilibrium
Propagation combines features of Contrastive Hebbian Learning and Contrastive
Divergence while solving the theoretical issues of both algorithms: the
algorithm computes the exact gradient of a well defined objective function.
Because the objective function is defined in terms of local perturbations, the
second phase of e-prop corresponds to only nudging the first-phase fixed point
towards a configuration that has lower cost value. In the case of a multi-layer
supervised neural network, the output units are slightly nudged towards their
target, and the perturbation introduced at the output layer propagates backward
in the network. The theory developed in this paper shows that the signal
'back-propagated' during this second phase actually contains information about
the error derivatives, which we use to implement a learning rule proved to
perform gradient descent with respect to the objective function. Thus, this
work makes it more plausible that a mechanism similar to backpropagation could
be implemented by brains.},
	url = {http://arxiv.org/pdf/1602.05179v4},
	eprint = {1602.05179v4},
	arxivid = {1602.05179v4},
	archiveprefix = {arXiv},
	month = {Feb},
	year = {2016},
	booktitle = {arXiv},
	title = {Equilibrium Propagation: Bridging the Gap Between Energy-Based Models
  and Backpropagation},
	author = {Benjamin Scellier and Yoshua Bengio}
}

