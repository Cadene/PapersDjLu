@article { Hudson2018,
	abstract = {We present the MAC network, a novel fully differentiable neural network
architecture, designed to facilitate explicit and expressive reasoning. Drawing
inspiration from first principles of computer organization, MAC moves away from
monolithic black-box neural architectures towards a design that encourages both
transparency and versatility. The model approaches problems by decomposing them
into a series of attention-based reasoning steps, each performed by a novel
recurrent Memory, Attention, and Composition (MAC) cell that maintains a
separation between control and memory. By stringing the cells together and
imposing structural constraints that regulate their interaction, MAC
effectively learns to perform iterative reasoning processes that are directly
inferred from the data in an end-to-end approach. We demonstrate the model's
strength, robustness and interpretability on the challenging CLEVR dataset for
visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the
error rate of the previous best model. More importantly, we show that the model
is computationally-efficient and data-efficient, in particular requiring 5x
less data than existing models to achieve strong results.},
	url = {http://arxiv.org/pdf/1803.03067v1},
	eprint = {1803.03067},
	arxivid = {1803.03067},
	archiveprefix = {arXiv},
	month = {Mar},
	year = {2018},
	booktitle = {arXiv},
	title = {{Compositional Attention Networks for Machine Reasoning}},
	author = {Drew A. Hudson and Christopher D. Manning}
}

