@article { Gordon2017,
	abstract = {We present MorphNet, an approach to automate the design of neural network
structures. MorphNet iteratively shrinks and expands a network, shrinking via a
resource-weighted sparsifying regularizer on activations and expanding via a
uniform multiplicative factor on all layers. In contrast to previous
approaches, our method is scalable to large networks, adaptable to specific
resource constraints (e.g. the number of floating-point operations per
inference), and capable of increasing the network's performance. When applied
to standard network architectures on a wide variety of datasets, our approach
discovers novel structures in each domain, obtaining higher performance while
respecting the resource constraint.},
	url = {http://arxiv.org/pdf/1711.06798v3},
	eprint = {1711.06798},
	arxivid = {1711.06798},
	archiveprefix = {arXiv},
	month = {Nov},
	year = {2017},
	booktitle = {arXiv},
	title = {{MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep
  Networks}},
	author = {Ariel Gordon and Elad Eban and Ofir Nachum and Bo Chen and Hao Wu and Tien-Ju Yang and Edward Choi}
}

