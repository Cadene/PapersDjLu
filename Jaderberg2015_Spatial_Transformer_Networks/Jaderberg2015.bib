@article { Jaderberg2015,
	abstract = {Convolutional Neural Networks define an exceptionally powerful class of
models, but are still limited by the lack of ability to be spatially invariant
to the input data in a computationally and parameter efficient manner. In this
work we introduce a new learnable module, the Spatial Transformer, which
explicitly allows the spatial manipulation of data within the network. This
differentiable module can be inserted into existing convolutional
architectures, giving neural networks the ability to actively spatially
transform feature maps, conditional on the feature map itself, without any
extra training supervision or modification to the optimisation process. We show
that the use of spatial transformers results in models which learn invariance
to translation, scale, rotation and more generic warping, resulting in
state-of-the-art performance on several benchmarks, and for a number of classes
of transformations.},
	url = {http://arxiv.org/pdf/1506.02025v3},
	eprint = {1506.02025},
	arxivid = {1506.02025},
	archiveprefix = {arXiv},
	month = {Jun},
	year = {2015},
	booktitle = {arXiv},
	title = {Spatial Transformer Networks},
	author = {Max Jaderberg and Karen Simonyan and Andrew Zisserman and Koray Kavukcuoglu}
}

