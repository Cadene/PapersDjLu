@article { Wang2016a,
	abstract = {This paper proposes the Stacked Approximated Regression Machine (SARM), a
novel, simple yet powerful deep learning (DL) baseline. We start by discussing
the relationship between regularized regression models and feed-forward
networks, with emphasis on the non-negative sparse coding and convolutional
sparse coding models. We demonstrate how these models are naturally converted
into a unified feed-forward network structure, which coincides with popular DL
components. SARM is constructed by stacking multiple unfolded and truncated
regression models. Compared to the PCANet, whose feature extraction layers are
completely linear, SARM naturally introduces non-linearities, by embedding
sparsity regularization. The parameters of SARM are easily obtained, by solving
a series of light-weight problems, e.g., PCA or KSVD. Extensive experiments are
conducted, which show that SARM outperforms the existing simple deep baseline,
PCANet, and is on par with many state-of-the-art deep models, but with much
lower computational loads.},
	url = {http://arxiv.org/pdf/1608.04062v1},
	eprint = {1608.04062v1},
	arxivid = {1608.04062v1},
	archiveprefix = {arXiv},
	month = {Aug},
	year = {2016},
	booktitle = {arXiv},
	title = {Stacked Approximated Regression Machine: A Simple Deep Learning Approach},
	author = {Zhangyang Wang and Shiyu Chang and Qing Ling and Shuai Huang and Xia Hu and Honghui Shi and Thomas S. Huang}
}

