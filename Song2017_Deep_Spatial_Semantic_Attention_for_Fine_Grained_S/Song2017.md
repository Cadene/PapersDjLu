[Supp](http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Song_Deep_Spatial-Semantic_Attention_ICCV_2017_supplemental.pdf)
[Downloads dataset and model](http://sketchx.eecs.qmul.ac.uk/downloads/)

# Fine Grained Sketch-Based Image Retrieval (FG-SBIR)

By Jifei Song*, Qian Yu*, Yi-The Song, Tao Xiang, Timothy M. Hospedales (2)

- Queen Mary University of London
- University of Edinburgh (2)

## Use case FG-SBIR

<img width=500 src="https://www.dropbox.com/s/uq64tkizvtriiag/FG-SBIR.png?raw=1" />

Searching online product catalogues for shoes, fourniture and hand bags by finger sketching on a smart-phone screen

## SOTA SBIR and FG-SBIR

<img width=500 src="https://www.dropbox.com/s/oyllumhrzgm1qrp/SBIR.png?raw=1" />

- Most existing SBIR works focus on category-level sketch-to-photo retrieval
    -  12 papers cited
    -  oldest from 2010
- First FG-SBIR with deformable part-based model (DPM) and graph matching (BMCV2014), 2 papers with deep learning (SIGGRAPH 2016, CVPR2016)

## Problem with FG-SBIR

- large domain gap between sketch and photo 
- sketch != edgemap -> global and local shape and spatial misalignments
- given one sketch, many visually similar candidate photos ; the correct match and wrong differ in some localised object parts


## Contributions

1. Novel deep FG-SBIR model with SelfAttention, Shortcuts and LayerFusion
    - learn discriminative feature representation spatially attended (SelfAttention)
    - and includes both coarse and fine details (Shortcuts and LayerFusion)
2. New higher order loss function with learnable parameters
    - more robust against feature misalignment and noise between domains
3. Biggest FG-SBIR dataset introduced
    - biggest number of sketch-photo pairs for a single object category
4. Significantly outperforms SOTA on three benchmarks


## Model

<img width=500 src="https://www.dropbox.com/s/qcsh7rwdpkqt8ru/model.png?raw=1" />

### Attention Layer

$$s_{i,j} = F_{att}(f_{i,j} ; \mathbf{W_a})$$
$$\alpha_{i,j} = softmax(s_{i,j})$$

$F_{att}(\dot)$ mapping fonction and $W_a$ its parameters. Here it is two convolutional layers with kernel size 1.

$$f^{att}_{i,j} = \alpha_{i,j} \odot f_{i,j}$$

To overcome the fact that $f_{att}$ could be corrupted by noise and loose any useful information in the original feature map $\mathbf{f}$:

$$\mathbf{f}^{att}_s = \mathbf{f} + \mathbf{\alpha} \odot \mathbf{f}$$

### Coarse-fine Fusion

$\mathbf{f}^{att}_s$ is spatially aware and attentive to fine-grained details.

Problem: These tend to be lost going through multiple subsequent fully connected layers.

Solution: 
$$\mathbf{f}^{final} = concat(\mathbf{f}^{att}_s , \mathbf{f}^{FC7})$$

### HOLEF Loss

#### Triplet Loss with First-order Energy Function

For a given triplet $t = (s, p^+, p^-)$ with sketch $s$, a positive photo $p^+$, a negative photo $p^-$.

$$L_\theta (s, p^+, p^-) = max(0, \Delta + D(F_\theta(s), F_\theta(p^+)) - D(F_\theta(s), F_\theta(p^-))$$

with $\Delta$ the margin, $F_\theta(\cdot)$ the output of the corresponding network branch, $D(\cdot,\cdot)$ the Euclidean distance.

#### Triplet Loss with a Higher-order Energy Function

Intuition: To compare two misaligned and noisy feature inputs, we can exploit higher order structural difference.

Proposition: Computing a 2nd order feature difference using outer subtraction.

Given two input vectors of $k$ dimensions, the outer subtraction $\ominus$ is a $k \times k$ matrix. For example, when $k = 3$:

<img width=500 src="https://www.dropbox.com/s/jx4gee9f0xpl0nt/higher-order.png?raw=1" />

Problem: Only a subset of the comparisons are expected to be useful.

Solution1: A weighting factor is introduced.

$$D_H(F_\theta(s), F_\theta(p)) = \sum (F_\theta(s) \ominus F_\theta(p))^{\circ 2} \cdot \mathbf{W}$$

where $\circ 2$ is the element-wise square and $\mathbf{W}$ is a $k \times k$ learnable weighting matrix.

Solution2: Two regularises are introduced to keep $\mathbf{W}$ in the vicinity of $\mathbf{I}$, but prevent $\mathbf{W} - \mathbf{I}$ from being extremely sparse (i.e. degenerating into a first-order loss).

$$L_\theta (s, p^+, p^-) = max(0, \Delta + D_H(F_\theta(s), F_\theta(p^+)) - D_H(F_\theta(s), F_\theta(p^-))$$
$$ + \lambda || \mathbf{W} _ \mathbf{I} ||_1 + \lambda || \mathbf{W} - \mathbf{I}||_F $$

with $\mathbf{I}$ the identity matrix, $||\cdot ||_F the Frobenious norm.

Note: Both Mahalanobis and proposed distance are 2nd order. But, in Mahalanobis, one first computes the element-wise subtraction $x - y$ and then the 2nd order bilinear product of the difference vectors. In other words , elements of different positions are not directly compared. Thus not suitable for dealing with fine-grained feature misalignment and noise.

$$D_M(x,y) = (x - y)^T M (x - y)$$
$$ = x^T M x + y^T M y - 2 x^T M y$$

## Experiments

### Datasets

#### QMUL-Shoe and QMUL-Chair

- QMUL-Shoe contain 419 shoe sketch-photo pairs
- QMUL-Chair contain 297 chair sketch-photo pairs
- real photo are collected from online shopping websites
- sketches are free-hand ones collected via crowdsourcing
- There are 13,680 and 9,000 human triplet annotations which are used to train the triplet model

#### Handbag

- 568 sketch-photo pairs
- handbags exhibit more complex visual patterns and deformable bodies than shoes and chairs
- Genereate triplets using only true and false matches, rather than exhaustive similarity ranking

### Implementation Details

- Each branch is pretrained using a sketch recognition dataset and ImageNet photo-edgemap pairs, before finetuning

### Metric

- The ratio of correctly prediction the true match at top-1 and at top-10 (acc. @1 and acc @10).

### Comparative Results

<img width=500 src="https://www.dropbox.com/s/3nnz6pusvz0yik5/table1.png?raw=1" />

### Ablation Study

#### Base vs CFF vs HOLEF

- Base: Triplet loss
- CFF:  Coarse Fine Fusion
- HOELF: Loss
- with shortcut self attention

<img width=500 src="https://www.dropbox.com/s/sed4k16usy8skwi/table2.png?raw=1" />

#### Effectiveness of attention

- with or without attention
- with shortcut

<img width=500 src="https://www.dropbox.com/s/vi2tq6k5s3irvgy/table3.png?raw=1" />

- with or without shortcut

<img width=500 src="https://www.dropbox.com/s/y05qgvnuctjhl0m/table4.png?raw=1" />

#### Alternative Triplet Losses

- with CFF ans shortcut self attention

<img width=500 src="https://www.dropbox.com/s/sjyp652uc4dal3d/table5.png?raw=1" />

### Qualitative results

#### Attention masks

<img width=500 src="https://www.dropbox.com/s/sglgybmv4eqq01l/quali_shoes.png?raw=1" />

<img width=500 src="https://www.dropbox.com/s/9srf3e743721xxu/quali_chairs.png?raw=1" />

<img width=500 src="https://www.dropbox.com/s/ij4p4228h2ik4rb/quali_handbags.png?raw=1" />

#### Versus Triplet

<img width=500 src="https://www.dropbox.com/s/v83uumov3joq81d/quali_compa_shoes.png?raw=1" />

<img width=500 src="https://www.dropbox.com/s/o2e8oiw7327e4l5/quali_compa_bags.png?raw=1" />

<img width=500 src="https://www.dropbox.com/s/ygd7w5xh3g71qvn/quali_compa_bags_2.png?raw=1" />


## Remarques:

They don't use the class information.
